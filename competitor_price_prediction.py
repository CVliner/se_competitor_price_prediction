# -*- coding: utf-8 -*-
"""SALES PREDICTION USING PYTHON

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/sales-prediction-using-python-b9ff7dff-6861-47cd-a67d-26b6bcc7b0fd.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240526/auto/storage/goog4_request%26X-Goog-Date%3D20240526T201318Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D34c17bbe17bc5ee03536768ca128bd5c0f61e7e1f9f1f27f8331275a265b7e5a84bba2cdf8e5274554b94534fdcae9b9ce4c854ed561c8651734bc7894e7e64efcb224abf70a72ef8e44958d4f82916301d543d98d33a90aef0174edceffb14af59179fcafc46fe7ee41045519e6b3c3266c68f93aedc1dd3687e3e4516fbbcf6da217f774e85165a38c26bc1f7d76a874a55b66980d04d635173a72c97619f82da7fb5a10d0f8d54af2b24c76ddf4cc8c102e84f190751792622a6f162e98ba54f900599d84f00365b061f6e80a5fb9e9d4ffd2eebe3c8aefec788a385d414750e4f4c9debb15debaaf45bb0ed0bbbf18729023cb0f7eb277d0c6ff793f861c

# Competitor Price Forecast

Competitor price prediction with Python enables businesses to forecast future sales based on factors like business expenses, customer segmentation and price positioning of item. Using ML-techniques, companies can analyze historical data, construct predictive models and make decisions to optimize marketing strategies, allocate resources efficiently and maximize sales. By timely updating the models with new data, commpany can adapt to market dynamics and achieve better performance in a competitive environment.
"""

# Import libraries

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
import statsmodels.formula.api as sm
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
import warnings

# Load dataset
warnings.simplefilter(action='ignore', category=FutureWarning)
os.getcwd()
df = pd.read_csv("/content/price_competitor.csv")

"""### Exploratory Data Analysis"""

# View the first few rows of the dataset

df.head()

# Get the column names of the dataset

df.columns

df

# Get the shape of the dataset (rows, columns)

df.shape

# Check information about the dataset, data types, and missing values

df.info()

# Get statistical summary of the numerical columns

df.describe().T

# Check for missing values in the dataset

df.isnull().values.any()
df.isnull().sum()

"""### Data Visualization"""

# sns.pairplot(df, x_vars='YEAR', y_vars=['SIEMENS_G120_055','SIEMENS_G120_075','SIEMENS_G120_22','VACON_20_055','VACON_20_075','VACON_20_22','Mitsubishi_FR_E_700_075','ABB_ACS150_075','ABB_ACS150_22','Danfoss_VLT_075','Danfoss_VLT_22'], kind="reg")
# Melt the dataframe to long format for easier plotting with FacetGrid
df_melted = df.melt(id_vars=['YEAR'], var_name='Company', value_name='Price(EURO)')

# Create a FacetGrid
g = sns.FacetGrid(df_melted, col="Company", col_wrap=4, height=4, aspect=1.5)
g.map(sns.regplot, 'YEAR', 'Price(EURO)')

# Adjust layout
plt.tight_layout()
plt.show()

# Histograms to check the normality assumption of the dependent variable (Sales)
# Create histograms with customized spacing
fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(15, 15))
df.hist(bins=20,ax=axes)

# Adjust spacing between plots
plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1, wspace=0.4, hspace=0.4)

plt.show()

# Linear regression plots to visualize the relationship between each independent variable and the dependent variable

sns.lmplot(x='SIEMENS_G120_055', y='YEAR', data=df)
sns.lmplot(x='SIEMENS_G120_075', y='YEAR', data=df)
sns.lmplot(x='SIEMENS_G120_22',y= 'YEAR', data=df)
sns.lmplot(x='VACON_20_055', y='YEAR', data=df)
sns.lmplot(x='VACON_20_075', y='YEAR', data=df)
sns.lmplot(x='VACON_20_22',y= 'YEAR', data=df)
sns.lmplot(x='Mitsubishi_FR_E_700_075', y='YEAR', data=df)
sns.lmplot(x='ABB_ACS150_075', y='YEAR', data=df)
sns.lmplot(x='ABB_ACS150_22',y= 'YEAR', data=df)
sns.lmplot(x='Danfoss_VLT_075', y='YEAR', data=df)
sns.lmplot(x='Danfoss_VLT_22',y= 'YEAR', data=df)

# Correlation Heatmap to check for multicollinearity among independent/dependent variables

corrmat = df.corr()
f, ax = plt.subplots(figsize=(12, 9))
sns.heatmap(corrmat, vmin=0, vmax=1, square=True, cmap="YlGnBu", ax=ax)
plt.show()

# Model Preparation

# Prepare features and target
X = df[['YEAR']]
future_years = np.arange(2021, 2023).reshape(-1, 1)

# Dictionary to store predictions
predictions = {'YEAR': np.arange(2021, 2023)}

# Train a linear regression model for each column and predict future values
for column in df.columns[1:]:
    y = df[[column]]

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train the model on training data
    model = LinearRegression().fit(X_train, y_train)

    # Optionally, you can evaluate the model on the test data
    score = model.score(X_test, y_test)
    print(f'R^2 score for {column}: {score:.2f}')
    # Print the model coefficients
    print(f'Coefficients for {column}: {model.coef_}')
    print(f'Intercept for {column}: {model.intercept_}')

    # Predict future values
    predictions[column] = model.predict(future_years).flatten()

# Convert predictions to DataFrame
predictions_df = pd.DataFrame(predictions)

# Combine with the original data
df_combined = pd.concat([df, predictions_df], ignore_index=True)

# Display the predictions
print(df_combined)

# Plotting the results
for column in df.columns[1:]:
    plt.figure()
    plt.plot(df['YEAR'], df[column], label='Historical Data')
    plt.plot(predictions_df['YEAR'], predictions_df[column], label='Predictions', linestyle='--')
    plt.xlabel('Year')
    plt.ylabel(column)
    plt.title(f'Prediction for {column}')
    plt.legend()
    plt.show()